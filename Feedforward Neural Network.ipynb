{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Feedforward Neural Network with the MNIST dataset\n",
    "\n",
    "This code is adapted from Michael Nielsen's online book [*Neural Networks and Deep Learning*](http://neuralnetworksanddeeplearning.com/index.html). There are a few conventions followed here that look slightly different from how we may understand neural networks. In particular, we usually think of passing data through a layer using a matrix multiplication. This code does this calculation element-wise, which is only done for computational efficiency. My goal is to present this code step-by-step, offering explanation where I can.\n",
    "\n",
    "To run each cell, press `Shift` + `Enter` when the cell is selected.\n",
    "\n",
    "## Loading the data\n",
    "\n",
    "These two functions will be called to load in our MNIST image data. Currently, there are 50,000 images stored in `mnist.pkl.gz` in binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These libraries are used to unzip and read in the binary data.\n",
    "import pickle\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "\n",
    "# This is the standard numerical library for Python.\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    \"\"\" Return a tuple containing (training_data, validation_data,\n",
    "    test_data). \n",
    "    \n",
    "    In particular, `training_data` is a list containing 50,000\n",
    "    2-tuples `(x, y)`.  \n",
    "    `x` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  \n",
    "    \n",
    "    `y` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for `x`.\n",
    "    \n",
    "    `validation_data` and `test_data` are lists containing 10,000\n",
    "    2-tuples `(x, y)`. In each case, `x` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and `y` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to `x`.\n",
    "    \n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unzip and load in the binary data.\n",
    "    with gzip.open('mnist.pkl.gz', 'rb') as binary_file:\n",
    "        tr_d, va_d, te_d = pickle.load(binary_file, encoding=\"latin1\")\n",
    "    \n",
    "    # Flatten each image into a vector.\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    \n",
    "    # Convert the class variable into e_j.\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    \n",
    "    # Pair the flattened images with their corresponding e_j.\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    \n",
    "    # Repeat the above process for validation and test data.\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    \n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    \n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\" Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's where we actually load in the data.\n",
    "training_data, validation_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the activation function and its derivative.\n",
    "\n",
    "For this network, we'll use the sigmoid activation function defined by\n",
    "$$\n",
    "    \\sigma(z) = \\frac{1}{1 + e^{-z}},\n",
    "$$\n",
    "with the derivative given by $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$.\n",
    "\n",
    "We can use other activation functions, and I've included a definition for the rectified linear unit (ReLU) function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\" The sigmoid function. \"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\" Derivative of the sigmoid function. \"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\" The rectified linear unit function. \"\"\"\n",
    "    return np.max(0, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the cost function.\n",
    "\n",
    "This code defines both the mean squared error and the cross-entropy cost functions. By default, our `Network` object will use the cross-entropy cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticCost(object):\n",
    "    # Calling these 'staticmethod's just allows them to be used outside of this class definition.\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\" Return the cost associated with an output `a` and desired output\n",
    "        `y`.\n",
    "        \"\"\"\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\" Return the error delta from the output layer. \"\"\"\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "\n",
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\" Return the cost associated with an output `a` and desired output\n",
    "        `y`.  \n",
    "        \n",
    "        Note that np.nan_to_num is used to ensure numerical\n",
    "        stability. In particular, if both `a` and `y` have a 1.0\n",
    "        in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "        returns nan. The np.nan_to_num ensures that that is converted\n",
    "        to the correct value (0.0).\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\" Return the error delta from the output layer.  \n",
    "        \n",
    "        Note that the parameter `z` is not used by the method. \n",
    "        It is included in the method's parameters in order to make the \n",
    "        interface consistent with the delta method for other cost classes.\n",
    "        \"\"\"\n",
    "        return (a-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the `Network` object\n",
    "\n",
    "If you're new to object-oriented programming, you can think of this step as declaring a custom data type. For example, instead of an `integer` or `string` class, we'll declare a `Network` class. This class will have **attributes** (variables defined within the class) and **methods** (functions defined within the class).\n",
    "\n",
    "I've tried to add comments to make this easier to understand. The red comments at the beginning of each method are the **docstrings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        \"\"\" Define the attributes for the Network class.\n",
    "        \n",
    "        The list `sizes` contains the number of neurons in the respective\n",
    "        layers of the network. For example, if the list was [2, 3, 1]\n",
    "        then it would be a three-layer network, with the first layer\n",
    "        containing 2 neurons, the second layer 3 neurons, and the\n",
    "        third layer 1 neuron. The biases and weights for the network\n",
    "        are initialized randomly, using\n",
    "        `self.default_weight_initializer` (see docstring for that\n",
    "        method).\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost = cost\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "        \"\"\" Initialize each weight using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1 divided by the square root of the number of\n",
    "        weights connecting to the same neuron. \n",
    "        \n",
    "        Initialize the biases using a Gaussian distribution with mean 0 and \n",
    "        standard deviation 1.\n",
    "\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "        \"\"\" Initialize the weights using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1. Initialize the biases using a\n",
    "        Gaussian distribution with mean 0 and standard deviation 1.\n",
    "\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "\n",
    "        This weight and bias initializer uses the same approach as in\n",
    "        Chapter 1, and is included for purposes of comparison.  It\n",
    "        will usually be better to use the default weight initializer\n",
    "        instead.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a, activation = sigmoid):\n",
    "        \"\"\" Return the output of the network if `a` is input. \"\"\"\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = activation(np.dot(w, a) + b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False,\n",
    "            early_stopping_n = 0):\n",
    "        \"\"\" Train the neural network using mini-batch stochastic gradient\n",
    "        descent.  The `training_data` is a list of tuples `(x, y)`\n",
    "        representing the training inputs and the desired outputs.  The\n",
    "        other non-optional parameters are self-explanatory, as is the\n",
    "        regularization parameter `lmbda`.  The method also accepts\n",
    "        `evaluation_data`, usually either the validation or test\n",
    "        data.  We can monitor the cost and accuracy on either the\n",
    "        evaluation data or the training data, by setting the\n",
    "        appropriate flags.  The method returns a tuple containing four\n",
    "        lists: the (per-epoch) costs on the evaluation data, the\n",
    "        accuracies on the evaluation data, the costs on the training\n",
    "        data, and the accuracies on the training data.  All values are\n",
    "        evaluated at the end of each training epoch.  So, for example,\n",
    "        if we train for 30 epochs, then the first element of the tuple\n",
    "        will be a 30-element list containing the cost on the\n",
    "        evaluation data at the end of each epoch. Note that the lists\n",
    "        are empty if the corresponding flag is not set.\n",
    "        \"\"\"\n",
    "\n",
    "        # early stopping functionality:\n",
    "        best_accuracy = 1\n",
    "\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        if evaluation_data:\n",
    "            evaluation_data = list(evaluation_data)\n",
    "            n_data = len(evaluation_data)\n",
    "\n",
    "        # early stopping functionality:\n",
    "        best_accuracy = 0\n",
    "        no_accuracy_change = 0\n",
    "\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
    "\n",
    "            print(\"Epoch %s training complete\" % j)\n",
    "\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print(\"Cost on training data: {}\".format(cost))\n",
    "                \n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on training data: {} / {}\".format(accuracy, n))\n",
    "                \n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print(\"Cost on evaluation data: {}\".format(cost))\n",
    "                \n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))\n",
    "\n",
    "            # Early stopping:\n",
    "            if early_stopping_n > 0:\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    no_accuracy_change = 0\n",
    "                    #print(\"Early-stopping: Best so far {}\".format(best_accuracy))\n",
    "                else:\n",
    "                    no_accuracy_change += 1\n",
    "\n",
    "                if (no_accuracy_change == early_stopping_n):\n",
    "                    #print(\"Early-stopping: No accuracy change in last epochs: {}\".format(early_stopping_n))\n",
    "                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
    "\n",
    "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\" Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        `mini_batch` is a list of tuples `(x, y)`, `eta` is the\n",
    "        learning rate, `lmbda` is the regularization parameter, and\n",
    "        `n` is the total size of the training data set.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\" Return a tuple `(nabla_b, nabla_w)` representing the\n",
    "        gradient for the cost function C_x.  `nabla_b` and\n",
    "        `nabla_w` are layer-by-layer lists of numpy arrays, similar\n",
    "        to `self.biases` and `self.weights`.\n",
    "        \"\"\"\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # Feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        \n",
    "        # TODO: Add in the option for the softmax function in the final layer.\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        # Backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        \n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            \n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "        \"\"\" Return the number of inputs in `data` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "\n",
    "        The flag `convert` should be set to False if the data set is\n",
    "        validation or test data (the usual case), and to True if the\n",
    "        data set is the training data. The need for this flag arises\n",
    "        due to differences in the way the results `y` are\n",
    "        represented in the different data sets.  In particular, it\n",
    "        flags whether we need to convert between the different\n",
    "        representations.  It may seem strange to use different\n",
    "        representations for the different data sets.  Why not use the\n",
    "        same representation for all three data sets?  It's done for\n",
    "        efficiency reasons -- the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "        These are different types of computations, and using different\n",
    "        representations speeds things up.  More details on the\n",
    "        representations can be found in the docstrings for `load_data`.\n",
    "        \"\"\"\n",
    "        \n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "\n",
    "        result_accuracy = sum(int(x == y) for (x, y) in results)\n",
    "        return result_accuracy\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert = False):\n",
    "        \"\"\" Return the total cost for the data set `data``.  \n",
    "        \n",
    "        The flag `convert` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the `accuracy` method, above.\n",
    "        \"\"\"\n",
    "        \n",
    "        cost = 0.0\n",
    "        \n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "            \n",
    "            # Add on the regularization cost.\n",
    "            cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights) # '**' - to the power of.\n",
    "            \n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\" Save the neural network to the file `filename`. \"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        \n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the Neural Network\n",
    "\n",
    "So far we haven't actually created our network, we've just set up the framework for doing so. We may now create an instance of a network and pass in our desired hyper-parameters.\n",
    "\n",
    "As an example, I'll initialize a `Network` instance with 3 layers of sizes 784, 30, and 10. Since the data are 28x28 pixels, the input layer has 28x28 = 784 units (nodes). The last layer has 10 units because there are 10 possible classes. \n",
    "\n",
    "The second layer can have any number of units, and we can add any number of layers with any number of units between the input layer and the output layer. Adding in extra layers will increase the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our network.\n",
    "net = Network([784, 30, 10], cost = CrossEntropyCost);\n",
    "\n",
    "# Initialize the weights and biases.\n",
    "net.default_weight_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to begin training. I'll specify 100 epochs, a mini-batch size of 10, the learning rate `eta` as 0.5, and the regularization parameter `lmbda` as 0.1.\n",
    "\n",
    "When I execute the next cell, the training will begin, and takes my computer (Intel i7-2600 CPU @ 3.40GHz, 4 Cores, 8 logical processors) about 12 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 9371 / 10000\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 9452 / 10000\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9580 / 10000\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9468 / 10000\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9572 / 10000\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9575 / 10000\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9515 / 10000\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9588 / 10000\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9584 / 10000\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9606 / 10000\n",
      "Epoch 10 training complete\n",
      "Accuracy on evaluation data: 9618 / 10000\n",
      "Epoch 11 training complete\n",
      "Accuracy on evaluation data: 9617 / 10000\n",
      "Epoch 12 training complete\n",
      "Accuracy on evaluation data: 9609 / 10000\n",
      "Epoch 13 training complete\n",
      "Accuracy on evaluation data: 9617 / 10000\n",
      "Epoch 14 training complete\n",
      "Accuracy on evaluation data: 9623 / 10000\n",
      "Epoch 15 training complete\n",
      "Accuracy on evaluation data: 9601 / 10000\n",
      "Epoch 16 training complete\n",
      "Accuracy on evaluation data: 9616 / 10000\n",
      "Epoch 17 training complete\n",
      "Accuracy on evaluation data: 9607 / 10000\n",
      "Epoch 18 training complete\n",
      "Accuracy on evaluation data: 9603 / 10000\n",
      "Epoch 19 training complete\n",
      "Accuracy on evaluation data: 9615 / 10000\n",
      "Epoch 20 training complete\n",
      "Accuracy on evaluation data: 9629 / 10000\n",
      "Epoch 21 training complete\n",
      "Accuracy on evaluation data: 9617 / 10000\n",
      "Epoch 22 training complete\n",
      "Accuracy on evaluation data: 9600 / 10000\n",
      "Epoch 23 training complete\n",
      "Accuracy on evaluation data: 9603 / 10000\n",
      "Epoch 24 training complete\n",
      "Accuracy on evaluation data: 9583 / 10000\n",
      "Epoch 25 training complete\n",
      "Accuracy on evaluation data: 9618 / 10000\n",
      "Epoch 26 training complete\n",
      "Accuracy on evaluation data: 9599 / 10000\n",
      "Epoch 27 training complete\n",
      "Accuracy on evaluation data: 9605 / 10000\n",
      "Epoch 28 training complete\n",
      "Accuracy on evaluation data: 9596 / 10000\n",
      "Epoch 29 training complete\n",
      "Accuracy on evaluation data: 9567 / 10000\n",
      "Epoch 30 training complete\n",
      "Accuracy on evaluation data: 9552 / 10000\n",
      "Epoch 31 training complete\n",
      "Accuracy on evaluation data: 9592 / 10000\n",
      "Epoch 32 training complete\n",
      "Accuracy on evaluation data: 9586 / 10000\n",
      "Epoch 33 training complete\n",
      "Accuracy on evaluation data: 9595 / 10000\n",
      "Epoch 34 training complete\n",
      "Accuracy on evaluation data: 9616 / 10000\n",
      "Epoch 35 training complete\n",
      "Accuracy on evaluation data: 9595 / 10000\n",
      "Epoch 36 training complete\n",
      "Accuracy on evaluation data: 9587 / 10000\n",
      "Epoch 37 training complete\n",
      "Accuracy on evaluation data: 9530 / 10000\n",
      "Epoch 38 training complete\n",
      "Accuracy on evaluation data: 9606 / 10000\n",
      "Epoch 39 training complete\n",
      "Accuracy on evaluation data: 9595 / 10000\n",
      "Epoch 40 training complete\n",
      "Accuracy on evaluation data: 9598 / 10000\n",
      "Epoch 41 training complete\n",
      "Accuracy on evaluation data: 9596 / 10000\n",
      "Epoch 42 training complete\n",
      "Accuracy on evaluation data: 9589 / 10000\n",
      "Epoch 43 training complete\n",
      "Accuracy on evaluation data: 9581 / 10000\n",
      "Epoch 44 training complete\n",
      "Accuracy on evaluation data: 9590 / 10000\n",
      "Epoch 45 training complete\n",
      "Accuracy on evaluation data: 9585 / 10000\n",
      "Epoch 46 training complete\n",
      "Accuracy on evaluation data: 9605 / 10000\n",
      "Epoch 47 training complete\n",
      "Accuracy on evaluation data: 9567 / 10000\n",
      "Epoch 48 training complete\n",
      "Accuracy on evaluation data: 9594 / 10000\n",
      "Epoch 49 training complete\n",
      "Accuracy on evaluation data: 9574 / 10000\n",
      "Epoch 50 training complete\n",
      "Accuracy on evaluation data: 9589 / 10000\n",
      "Epoch 51 training complete\n",
      "Accuracy on evaluation data: 9597 / 10000\n",
      "Epoch 52 training complete\n",
      "Accuracy on evaluation data: 9601 / 10000\n",
      "Epoch 53 training complete\n",
      "Accuracy on evaluation data: 9594 / 10000\n",
      "Epoch 54 training complete\n",
      "Accuracy on evaluation data: 9602 / 10000\n",
      "Epoch 55 training complete\n",
      "Accuracy on evaluation data: 9574 / 10000\n",
      "Epoch 56 training complete\n",
      "Accuracy on evaluation data: 9567 / 10000\n",
      "Epoch 57 training complete\n",
      "Accuracy on evaluation data: 9597 / 10000\n",
      "Epoch 58 training complete\n",
      "Accuracy on evaluation data: 9588 / 10000\n",
      "Epoch 59 training complete\n",
      "Accuracy on evaluation data: 9586 / 10000\n",
      "Epoch 60 training complete\n",
      "Accuracy on evaluation data: 9605 / 10000\n",
      "Epoch 61 training complete\n",
      "Accuracy on evaluation data: 9591 / 10000\n",
      "Epoch 62 training complete\n",
      "Accuracy on evaluation data: 9582 / 10000\n",
      "Epoch 63 training complete\n",
      "Accuracy on evaluation data: 9595 / 10000\n",
      "Epoch 64 training complete\n",
      "Accuracy on evaluation data: 9583 / 10000\n",
      "Epoch 65 training complete\n",
      "Accuracy on evaluation data: 9569 / 10000\n",
      "Epoch 66 training complete\n",
      "Accuracy on evaluation data: 9580 / 10000\n",
      "Epoch 67 training complete\n",
      "Accuracy on evaluation data: 9554 / 10000\n",
      "Epoch 68 training complete\n",
      "Accuracy on evaluation data: 9576 / 10000\n",
      "Epoch 69 training complete\n",
      "Accuracy on evaluation data: 9578 / 10000\n",
      "Epoch 70 training complete\n",
      "Accuracy on evaluation data: 9575 / 10000\n",
      "Epoch 71 training complete\n",
      "Accuracy on evaluation data: 9585 / 10000\n",
      "Epoch 72 training complete\n",
      "Accuracy on evaluation data: 9581 / 10000\n",
      "Epoch 73 training complete\n",
      "Accuracy on evaluation data: 9591 / 10000\n",
      "Epoch 74 training complete\n",
      "Accuracy on evaluation data: 9584 / 10000\n",
      "Epoch 75 training complete\n",
      "Accuracy on evaluation data: 9585 / 10000\n",
      "Epoch 76 training complete\n",
      "Accuracy on evaluation data: 9569 / 10000\n",
      "Epoch 77 training complete\n",
      "Accuracy on evaluation data: 9584 / 10000\n",
      "Epoch 78 training complete\n",
      "Accuracy on evaluation data: 9584 / 10000\n",
      "Epoch 79 training complete\n",
      "Accuracy on evaluation data: 9604 / 10000\n",
      "Epoch 80 training complete\n",
      "Accuracy on evaluation data: 9572 / 10000\n",
      "Epoch 81 training complete\n",
      "Accuracy on evaluation data: 9587 / 10000\n",
      "Epoch 82 training complete\n",
      "Accuracy on evaluation data: 9549 / 10000\n",
      "Epoch 83 training complete\n",
      "Accuracy on evaluation data: 9562 / 10000\n",
      "Epoch 84 training complete\n",
      "Accuracy on evaluation data: 9581 / 10000\n",
      "Epoch 85 training complete\n",
      "Accuracy on evaluation data: 9582 / 10000\n",
      "Epoch 86 training complete\n",
      "Accuracy on evaluation data: 9572 / 10000\n",
      "Epoch 87 training complete\n",
      "Accuracy on evaluation data: 9572 / 10000\n",
      "Epoch 88 training complete\n",
      "Accuracy on evaluation data: 9561 / 10000\n",
      "Epoch 89 training complete\n",
      "Accuracy on evaluation data: 9582 / 10000\n",
      "Epoch 90 training complete\n",
      "Accuracy on evaluation data: 9578 / 10000\n",
      "Epoch 91 training complete\n",
      "Accuracy on evaluation data: 9580 / 10000\n",
      "Epoch 92 training complete\n",
      "Accuracy on evaluation data: 9574 / 10000\n",
      "Epoch 93 training complete\n",
      "Accuracy on evaluation data: 9569 / 10000\n",
      "Epoch 94 training complete\n",
      "Accuracy on evaluation data: 9571 / 10000\n",
      "Epoch 95 training complete\n",
      "Accuracy on evaluation data: 9577 / 10000\n",
      "Epoch 96 training complete\n",
      "Accuracy on evaluation data: 9574 / 10000\n",
      "Epoch 97 training complete\n",
      "Accuracy on evaluation data: 9579 / 10000\n",
      "Epoch 98 training complete\n",
      "Accuracy on evaluation data: 9579 / 10000\n",
      "Epoch 99 training complete\n",
      "Accuracy on evaluation data: 9562 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 100, 10, eta = 0.5, lmbda = 0.1, evaluation_data = test_data, monitor_evaluation_accuracy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we see that after 100 epochs we obtained 95.65\\% accuracy on the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make it possible to test individual cases or a collection of external test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
